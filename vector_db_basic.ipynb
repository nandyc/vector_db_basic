{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO95XxkmZhfouhGcskTw44Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MXO6W6RFdh77","executionInfo":{"status":"ok","timestamp":1724578922166,"user_tz":-330,"elapsed":13755,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"8f7cce49-516e-4c8a-fb05-8202da75a093"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n","Collecting openai\n","  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting jiter<1,>=0.4.0 (from openai)\n","  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n","Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 openai-1.42.0\n"]}],"source":["%pip install --upgrade tiktoken\n","%pip install --upgrade openai"]},{"cell_type":"code","source":["import tiktoken"],"metadata":{"id":"toVqJ_9ednrg","executionInfo":{"status":"ok","timestamp":1724578922167,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["encoding = tiktoken.get_encoding(\"cl100k_base\")\n"],"metadata":{"id":"A8GO1Xl3dqKX","executionInfo":{"status":"ok","timestamp":1724578923965,"user_tz":-330,"elapsed":1802,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(encoding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeVpJfguwtnU","executionInfo":{"status":"ok","timestamp":1724578923965,"user_tz":-330,"elapsed":6,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"dcbc1002-bf02-45d2-e2ce-13bcbef86a64"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<Encoding 'cl100k_base'>\n"]}]},{"cell_type":"code","source":["encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"],"metadata":{"id":"jnGrtNI3dsWG","executionInfo":{"status":"ok","timestamp":1724578923965,"user_tz":-330,"elapsed":6,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(encoding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNUgSMHSwxUa","executionInfo":{"status":"ok","timestamp":1724578923965,"user_tz":-330,"elapsed":5,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"71c420db-0b2d-40a3-92ff-99eeedf9e48c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<Encoding 'cl100k_base'>\n"]}]},{"cell_type":"code","source":["encoding.encode(\"tiktoken is great!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmbBhN4ydyZ0","executionInfo":{"status":"ok","timestamp":1724578923965,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"a5f5d1c4-e3b7-4cb7-f025-e25393e5f1b6"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[83, 1609, 5963, 374, 2294, 0]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":[],"metadata":{"id":"Xh_nhbKL0eYu","executionInfo":{"status":"ok","timestamp":1724578923966,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def num_tokens_from_string(string: str, encoding_name: str) -> int:\n","    \"\"\"Returns the number of tokens in a text string.\"\"\"\n","    encoding = tiktoken.get_encoding(encoding_name)\n","    num_tokens = len(encoding.encode(string))\n","    token_list = encoding.encode(string)\n","    print(f\"num_tokens: {num_tokens}\")\n","    print(f\"token_list: {token_list}\")\n","    return num_tokens\n"],"metadata":{"id":"Lhah25jKd3Ga","executionInfo":{"status":"ok","timestamp":1724580779991,"user_tz":-330,"elapsed":444,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["num_tokens_from_string(\"What is human life expectancy in the United States?\", \"cl100k_base\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2z3NsVMxgQp","executionInfo":{"status":"ok","timestamp":1724589579539,"user_tz":-330,"elapsed":454,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"a1bc1c15-8520-4623-9ff9-3fe592dad64a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["num_tokens: 10\n","token_list: [3923, 374, 3823, 2324, 66995, 304, 279, 3723, 4273, 30]\n"]},{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["encoding.decode([83, 1609, 5963, 374, 2294, 0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"pGWBA_Dhd5Fi","executionInfo":{"status":"ok","timestamp":1724509470438,"user_tz":-330,"elapsed":426,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"6f04c3a2-6071-481e-8560-927610520bf4"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'tiktoken is great!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-j_I1ur1d9-2","executionInfo":{"status":"ok","timestamp":1724585861171,"user_tz":-330,"elapsed":471,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"88ab5ae0-16bb-4468-e13c-4494b418e919"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[b't']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["def compare_encodings(example_string: str) -> None:\n","    \"\"\"Prints a comparison of three string encodings.\"\"\"\n","    # print the example string\n","    print(f'\\nExample string: \"{example_string}\"')\n","    # for each encoding, print the # of tokens, the token integers, and the token bytes\n","    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\"]:\n","        encoding = tiktoken.get_encoding(encoding_name)\n","        token_integers = encoding.encode(example_string)\n","        num_tokens = len(token_integers)\n","        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n","        print()\n","        print(f\"{encoding_name}: {num_tokens} tokens\")\n","        print(f\"token integers: {token_integers}\")\n","        print(f\"token bytes: {token_bytes}\")\n",""],"metadata":{"id":"2coVUnjwd-fu","executionInfo":{"status":"ok","timestamp":1724585916084,"user_tz":-330,"elapsed":2,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["compare_encodings(\"What is human life expectancy in the United States?\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWDXI9gGd_8P","executionInfo":{"status":"ok","timestamp":1724588272927,"user_tz":-330,"elapsed":437,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"4acb3f77-26c7-4026-ee20-8637bd242c4f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Example string: \"What is human life expectancy in the United States?\"\n","\n","r50k_base: 10 tokens\n","token integers: [2061, 318, 1692, 1204, 29098, 287, 262, 1578, 1829, 30]\n","token bytes: [b'What', b' is', b' human', b' life', b' expectancy', b' in', b' the', b' United', b' States', b'?']\n","\n","p50k_base: 10 tokens\n","token integers: [2061, 318, 1692, 1204, 29098, 287, 262, 1578, 1829, 30]\n","token bytes: [b'What', b' is', b' human', b' life', b' expectancy', b' in', b' the', b' United', b' States', b'?']\n","\n","cl100k_base: 10 tokens\n","token integers: [3923, 374, 3823, 2324, 66995, 304, 279, 3723, 4273, 30]\n","token bytes: [b'What', b' is', b' human', b' life', b' expectancy', b' in', b' the', b' United', b' States', b'?']\n"]}]},{"cell_type":"code","source":["compare_encodings(\"2 + 2 = 4\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XG5i8nbueBOW","executionInfo":{"status":"ok","timestamp":1724504356496,"user_tz":-330,"elapsed":484,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"322c9dff-ddfb-4bc4-fdf4-db0470e4ae02"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Example string: \"2 + 2 = 4\"\n","\n","r50k_base: 5 tokens\n","token integers: [17, 1343, 362, 796, 604]\n","token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n","\n","p50k_base: 5 tokens\n","token integers: [17, 1343, 362, 796, 604]\n","token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n","\n","cl100k_base: 7 tokens\n","token integers: [17, 489, 220, 17, 284, 220, 19]\n","token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"]}]},{"cell_type":"code","source":["compare_encodings(\"お誕生日おめでとう\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MdEdNAdeDHv","executionInfo":{"status":"ok","timestamp":1724504361648,"user_tz":-330,"elapsed":438,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"d40a1fd0-37b0-44eb-98ae-31231b0e9f40"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Example string: \"お誕生日おめでとう\"\n","\n","r50k_base: 14 tokens\n","token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n","token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n","\n","p50k_base: 14 tokens\n","token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n","token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n","\n","cl100k_base: 9 tokens\n","token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n","token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"]}]},{"cell_type":"code","source":["def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n","    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n","    try:\n","        encoding = tiktoken.encoding_for_model(model)\n","    except KeyError:\n","        print(\"Warning: model not found. Using cl100k_base encoding.\")\n","        encoding = tiktoken.get_encoding(\"cl100k_base\")\n","    if model in {\n","        \"gpt-3.5-turbo-0613\",\n","        \"gpt-3.5-turbo-16k-0613\",\n","        \"gpt-4-0314\",\n","        \"gpt-4-32k-0314\",\n","        \"gpt-4-0613\",\n","        \"gpt-4-32k-0613\",\n","        }:\n","        tokens_per_message = 3\n","        tokens_per_name = 1\n","    elif model == \"gpt-3.5-turbo-0301\":\n","        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n","        tokens_per_name = -1  # if there's a name, the role is omitted\n","    elif \"gpt-3.5-turbo\" in model:\n","        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n","        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n","    elif \"gpt-4\" in model:\n","        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n","        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n","    else:\n","        raise NotImplementedError(\n","            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n","        )\n","    num_tokens = 0\n","    for message in messages:\n","        num_tokens += tokens_per_message\n","        for key, value in message.items():\n","            num_tokens += len(encoding.encode(value))\n","            if key == \"name\":\n","                num_tokens += tokens_per_name\n","    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n","    return num_tokens\n"],"metadata":{"id":"TU-sccYfeEY4","executionInfo":{"status":"ok","timestamp":1724504370234,"user_tz":-330,"elapsed":437,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# let's verify the function above matches the OpenAI API response\n","\n","from openai import OpenAI\n","import os\n","\n","client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n","\n","example_messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n","    },\n","    {\n","        \"role\": \"system\",\n","        \"name\": \"example_user\",\n","        \"content\": \"New synergies will help drive top-line growth.\",\n","    },\n","    {\n","        \"role\": \"system\",\n","        \"name\": \"example_assistant\",\n","        \"content\": \"Things working well together will increase revenue.\",\n","    },\n","    {\n","        \"role\": \"system\",\n","        \"name\": \"example_user\",\n","        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n","    },\n","    {\n","        \"role\": \"system\",\n","        \"name\": \"example_assistant\",\n","        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n","    },\n","]\n","\n","for model in [\n","    \"gpt-3.5-turbo-0301\",\n","    \"gpt-3.5-turbo-0613\",\n","    \"gpt-3.5-turbo\",\n","    \"gpt-4-0314\",\n","    \"gpt-4-0613\",\n","    \"gpt-4\",\n","    ]:\n","    print(model)\n","    # example token count from the function defined above\n","    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n","    # example token count from the OpenAI API\n","    response = client.chat.completions.create(model=model,\n","    messages=example_messages,\n","    temperature=0,\n","    max_tokens=1)\n","    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"yP6lOauReGfA","executionInfo":{"status":"error","timestamp":1724504380561,"user_tz":-330,"elapsed":3660,"user":{"displayName":"Nandan Chhabra","userId":"12230066684324505255"}},"outputId":"274526da-c9d1-4a90-b537-31ca222a8adf"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["gpt-3.5-turbo-0301\n","127 prompt tokens counted by num_tokens_from_messages().\n"]},{"output_type":"error","ename":"AuthenticationError","evalue":"Error code: 401 - {'error': {'message': 'Incorrect API key provided: <your Op*******************************var>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-da5ad022ce78>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# example token count from the OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     response = client.chat.completions.create(model=model,\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    666\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    667\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         )\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 937\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         return self._process_response(\n","\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <your Op*******************************var>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qRTCbxKXeIN4"},"execution_count":null,"outputs":[]}]}